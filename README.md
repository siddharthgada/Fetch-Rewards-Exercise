# Fetch Rewards Data Modeling & Analysis
This repository contains my solutions for the Fetch Rewards Data Modeling & Analysis Challenge, which involved structuring unstructured JSON data, writing SQL queries, identifying data quality issues, and communicating insights to stakeholders.

ğŸš€ Project Overview
This project required transforming raw, unstructured JSON data into a structured relational data model, writing business-focused SQL queries, and evaluating data quality issues. The key objectives included:

âœ”ï¸ Data Modeling: Designing a structured relational database schema based on provided sample data.
âœ”ï¸ SQL Queries: Answering business questions using optimized queries.
âœ”ï¸ Data Quality Assessment: Identifying potential inconsistencies, missing values, and anomalies.
âœ”ï¸ Stakeholder Communication: Writing a clear and concise business report.

ğŸ“‚ Project Deliverables

1. Relational Data Model
Reviewed unstructured JSON data and created a structured relational ER Diagram.
Defined tables, fields, primary keys, and foreign keys for optimal data organization.
Diagram Link: ER Diagram

2. Business-Focused SQL Queries
SQL queries were written to answer the following key business questions:
âœ… Top 5 brands by receipts scanned for the most recent month.
âœ… Average spend comparison for 'Accepted' vs. 'Rejected' receipts.
âœ… Total items purchased comparison for 'Accepted' vs. 'Rejected' receipts.
âœ… Brand with the highest spend among users created within the past 6 months.
âœ… Brand with the most transactions among users created within the past 6 months.

3. Data Quality Assessment
Using Python and SQL, I analyzed the dataset for potential data quality issues, including:
ğŸš¨ Missing or inconsistent data in key fields.
ğŸš¨ Duplicate records and anomalies.
ğŸš¨ Inconsistent date formats and incorrect timestamps.
ğŸš¨ Irregularities in rewardsReceiptStatus values.

ğŸ“Š Technologies & Tools Used
SQL: Querying and analyzing structured data.
Python (Pandas, NumPy): Data exploration, anomaly detection, and validation.
Database Modeling: Designing relational schema for structured storage.
Diagramming Tools (Visio): ER diagram creation for data structure visualization.
Google BigQuery: Database creation and querying

ğŸ” Key Learning Outcomes
ğŸ“Œ Data Modeling & Normalization:
  - Learned how to analyze raw JSON data and design a relational schema for a data warehouse.
  - Understood the importance of primary and foreign keys in maintaining data integrity.
ğŸ“Œ SQL Query Optimization:
  - Gained experience in writing efficient SQL queries to extract business insights.
  - Applied ranking functions, aggregation, and filtering to answer real-world business questions.
ğŸ“Œ Data Quality Analysis:
  - Discovered common data quality issues, such as duplicate entries, missing values, and format inconsistencies.
  - Used Python (Pandas, NumPy) and SQL to identify and address anomalies in the dataset.
ğŸ“Œ Effective Communication:
  - Developed clear and concise business reports and email communication for non-technical stakeholders.
  - Practised translating technical findings into actionable business recommendations.
